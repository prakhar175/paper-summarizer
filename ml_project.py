# -*- coding: utf-8 -*-
"""ML Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17FUN51awygRqngDQNsFNtkEtsDmZPw-7
"""

pip install PyMuPDF

import os
import re
import fitz
from transformers import pipeline
from google.colab import drive
drive.mount('/content/drive')

# Load LED summarizer
summarizer = pipeline("summarization", model="pszemraj/led-large-book-summary", tokenizer="pszemraj/led-large-book-summary")

# Load Biomedical NER
ner_pipeline = pipeline("ner", model="d4data/biomedical-ner-all", aggregation_strategy="simple")

base_path = "/content/drive/MyDrive/ML_PROJECT/Diseases"
disease_name = input("Enter the disease name to get the summary: ")
disease_folder = os.path.join(base_path, disease_name)
if os.path.isdir(disease_folder):
    print(f"Found folder for {disease_name}")
else:
    print(f"Folder for disease '{disease_name}' does not exist.")

def clean_text(text):
    text = re.sub(r"(Creative Commons.*?)(?:\n|$)", "", text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)#non-ASCII
    text = re.sub(r'\s+', ' ', text)#Spaces
    return text.strip()

def summarize_text(text):
    if len(text) < 10:
        return " Not enough text to summarize."
    summary_output = summarizer(text[:4000], max_length=500, min_length=50, do_sample=False)
    return summary_output[0]['summary_text']

# Loop through files and process each
for file_name in os.listdir(disease_folder):
    if file_name.endswith(".pdf"):
        file_path = os.path.join(disease_folder, file_name)
        # Extract text
        raw_text = ""
        if file_name.endswith(".txt"):
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_text = f.read()
        elif file_name.endswith(".pdf"):
            with fitz.open(file_path) as doc:
                for page in doc:
                    raw_text += page.get_text()
        cleaned_text = clean_text(raw_text)
        if len(cleaned_text) == 0:
            print(f"No text found for file with name: {file_name}")
            continue
        summary = summarize_text(cleaned_text)
        print(f"\nsummary of paper with name: {file_name} is")
        print(f"\n Summary:\n{summary}")
        ner_results = ner_pipeline(cleaned_text)